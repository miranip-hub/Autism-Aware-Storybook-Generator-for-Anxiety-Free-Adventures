{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13902920,"sourceType":"datasetVersion","datasetId":8857688}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Autonomous Kid Storybook Generator for Anxiety Relief\n\n# This Python 3 environment is set up for AI-powered story generation and image creation\n# It uses Google Gemini AI for generating kid-friendly stories and Stable Diffusion for illustrations\n# The system is designed to help children, including those with autism, reduce anxiety \n# by providing engaging, calming, and personalized story experiences\n\n# The notebook combines several components:\n# - Retrieval-Augmented Generation (RAG) to fetch relevant story topics\n# - Sentence Transformers and FAISS for semantic search\n# - Stable Diffusion for creating colorful, child-friendly images\n# - Gradio for an interactive UI where kids can explore scenes and provide feedback\n\n# Input: Children can type a place or scenario they like\n# Output: The system generates a story with multiple scenes and images\n# Feedback: Children can indicate if they enjoyed the story; happy feedback is saved to improve future stories\n\n# This notebook also handles API secrets securely via Kaggle Secrets\n# and supports GPU acceleration for faster image generation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1Ô∏è‚É£ Install required Python libraries for AI models, embeddings, and web interface\n!pip install --quiet langgraph           # Graph-based workflow library\n!pip install --quiet faiss-cpu          # Library for fast similarity search\n!pip install --quiet sentence-transformers  # Pretrained models for sentence embeddings\n!pip install --quiet diffusers transformers accelerate safetensors  # Stable Diffusion and supporting libraries\n!pip install gradio --upgrade           # Gradio library for building the web interface\n\n# 2Ô∏è‚É£ Import necessary libraries for generating images with Stable Diffusion\nfrom diffusers import StableDiffusionPipeline  # Stable Diffusion model pipeline\nimport torch                                   # PyTorch for tensor computations and GPU acceleration\n\n# 3Ô∏è‚É£ Load the Stable Diffusion model\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",  # Specify the pre-trained Stable Diffusion model\n    torch_dtype=torch.float16           # Use half-precision (FP16) to save GPU memory and improve speed\n).to(\"cuda\")                            # Move the model to GPU for faster image generation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T14:36:20.786774Z","iopub.execute_input":"2025-12-02T14:36:20.787018Z","iopub.status.idle":"2025-12-02T14:38:47.136278Z","shell.execute_reply.started":"2025-12-02T14:36:20.786992Z","shell.execute_reply":"2025-12-02T14:38:47.135610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- IMPORTS ----------------\nimport os, hashlib, sqlite3, json, threading\nfrom typing import Dict, Any\nfrom PIL import Image\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nimport google.generativeai as genai\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom langgraph.graph import StateGraph, END\n\n# ---------------- CONFIG ----------------\nENABLE_SD = True\nSD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\nSD_CACHE_PATH = \"/root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5\"\nDB_PATH = \"/kaggle/working/kid_feedback.db\"\nRAG_PATH = \"/kaggle/input/safety-rules1/kid_topics_enhanced1.json\"\n\n# ---------------- GEMINI AI CONFIG ----------------\nfrom kaggle_secrets import UserSecretsClient\ntry:\n    GEMINI_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n    genai.configure(api_key=GEMINI_API_KEY)\nexcept Exception as e:\n    raise RuntimeError(f\"Authentication Error: {e}\")\n\n# ---------------- SETUP STABLE DIFFUSION ----------------\nsd_pipe = None\nif ENABLE_SD:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    dtype = torch.float16 if device==\"cuda\" else torch.float32\n    try:\n        sd_pipe = StableDiffusionPipeline.from_pretrained(\n            SD_MODEL_ID, torch_dtype=dtype, safety_checker=None\n        ).to(device)\n    except Exception as e:\n        print(f\"[ERROR] SD pipeline failed: {e}\")\n        sd_pipe = None\n\n# ---------------- DATABASE ----------------\nconn = sqlite3.connect(DB_PATH, check_same_thread=False)\nc = conn.cursor()\nc.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS feedback (\n    id INTEGER PRIMARY KEY,\n    place TEXT,\n    scene_number INTEGER,\n    story TEXT,\n    image_path TEXT,\n    emotion TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\"\"\")\nconn.commit()\n\n# ---------------- LOAD RAG DATA ----------------\nif not os.path.exists(RAG_PATH):\n    raise FileNotFoundError(f\"RAG file not found: {RAG_PATH}\")\n\nwith open(RAG_PATH, \"r\", encoding=\"utf-8\") as f:\n    rag_data = json.load(f)\ntopics = [t.get(\"topic\",\"\") for t in rag_data]\n\nembed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\ntopic_embeddings = embed_model.encode(topics, convert_to_numpy=True)\nd = topic_embeddings.shape[1]\nfaiss.normalize_L2(topic_embeddings)\nindex = faiss.IndexFlatIP(d)\nindex.add(topic_embeddings)\n\n# ---------------- HELPER FUNCTIONS ----------------\ndef rag_search(place_name: str, k=1) -> Dict[str, Any]:\n    if not place_name.strip(): return {\"scene_prompt\": \"No place provided.\", \"tools_prompt\": \"\"}\n    q = embed_model.encode([place_name], convert_to_numpy=True)\n    faiss.normalize_L2(q)\n    D,I = index.search(q,k)\n    if I.size==0 or I[0][0]==-1: return {\"scene_prompt\": \"No matching topic found.\", \"tools_prompt\": \"\"}\n    return rag_data[int(I[0][0])]\n\ndef generate_story(scene_desc: str, state: Dict[str, Any]) -> str:\n    try:\n        model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n        prompt = f\"You are a friendly children's storyteller. Scene: {scene_desc}\"\n        resp = model.generate_content(prompt)\n        return getattr(resp,\"text\",str(resp)).strip()\n    except Exception as e:\n        state['error_flag'] = True\n        state['error_message'] = f\"Story generation failed: {e}\"\n        return \"\"\n\ndef auto_prompt_for_sd(scene_desc: str, state: Dict[str, Any]) -> str:\n    try:\n        model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n        prompt_resp = model.generate_content(f\"Convert this scene into a colorful, kid-friendly illustration: {scene_desc}\")\n        return getattr(prompt_resp,\"text\",str(prompt_resp)).strip()\n    except Exception as e:\n        state['error_flag'] = True\n        state['error_message'] = f\"SD prompt generation failed: {e}\"\n        return scene_desc\n\ndef sd_generate_autonomous(scene_desc: str, state: Dict[str, Any]) -> str:\n    if not ENABLE_SD or sd_pipe is None: return \"/kaggle/working/placeholder.png\"\n    prompt = auto_prompt_for_sd(scene_desc,state)\n    guidance = 7.5 if \"calm\" in prompt.lower() else 9.0\n    steps = 25 if \"simple\" in prompt.lower() else 40\n    try:\n        out = sd_pipe(prompt, num_inference_steps=steps, guidance_scale=guidance)\n        img = out.images[0]\n        fname = f\"/kaggle/working/scene_{hashlib.md5(prompt.encode()).hexdigest()[:8]}.png\"\n        img.save(fname)\n        return fname\n    except Exception as e:\n        state['error_flag'] = True\n        state['error_message'] = f\"SD generation failed: {e}\"\n        return \"/kaggle/working/placeholder.png\"\n\ndef db_save_scene(place: str, story: str, img: str, scene_number: int, emotion=\"excited\"):\n    c.execute(\n        \"INSERT INTO feedback(place,scene_number,story,image_path,emotion) VALUES (?,?,?,?,?)\",\n        (place, scene_number, story, img, emotion)\n    )\n    conn.commit()\n\ndef db_get_all_scenes(place: str):\n    c.execute(\"SELECT story, image_path FROM feedback WHERE place=? ORDER BY scene_number\", (place,))\n    rows = c.fetchall()\n    if rows:\n        return [{\"story\": row[0], \"image\": row[1], \"ready\": True} for row in rows]\n    return None\n\n# ---------------- GRAPH NODES ----------------\ndef node_check_db(state):\n    found = db_get_all_scenes(state.get(\"place\",\"\"))\n    if found:\n        state['found_in_db'] = True\n        state['storybook'] = found\n    else:\n        state['found_in_db'] = False\n    return state\n\ndef node_determine_scenes(state):\n    try:\n        model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n        prompt = f\"Decide how many kid-friendly story scenes to generate for {state.get('place','')}. Respond with a number.\"\n        resp = model.generate_content(prompt)\n        n = int(resp.text.strip())\n        state['num_scenes'] = max(1, min(n,5))\n    except:\n        state['num_scenes'] = 2\n    return state\n\ndef node_rag(state):\n    if state.get(\"error_flag\") or state.get(\"found_in_db\"): return state\n    state['scenes'] = []\n    for _ in range(state['num_scenes']):\n        rag_entry = rag_search(state.get(\"place\",\"\"))\n        scene_prompt = rag_entry.get(\"scene_prompt\",\"A fun scene\")\n        state['scenes'].append({'scene_desc': scene_prompt})\n    return state\n\ndef node_generate_story_and_image(state):\n    \"\"\"Generate story + images asynchronously using readiness flags.\"\"\"\n    if state.get(\"error_flag\"):\n        return state\n\n    scenes = state.get('scenes', [])\n    state['storybook'] = []\n\n    if not scenes:\n        return state\n\n    # First scene synchronous\n    first = scenes[0]\n    story = generate_story(first['scene_desc'], state)\n    img = sd_generate_autonomous(first['scene_desc'], state)\n    first['story'] = story\n    first['image'] = img\n    first['ready'] = True\n    state['storybook'].append(first)\n\n    # Background thread for remaining\n    def generate_remaining():\n        for scene in scenes[1:]:\n            try:\n                s = generate_story(scene['scene_desc'], state)\n                i = sd_generate_autonomous(scene['scene_desc'], state)\n                scene['story'] = s\n                scene['image'] = i\n                scene['ready'] = True\n                state['storybook'].append(scene)\n            except:\n                scene['story'] = \"‚ö†Ô∏è Error generating scene.\"\n                scene['image'] = \"\"\n                scene['ready'] = True\n                state['storybook'].append(scene)\n\n    threading.Thread(target=generate_remaining, daemon=True).start()\n    return state\n\ndef node_save_if_req(state):\n    return state\n\n# ---------------- GRAPH SETUP ----------------\ngraph = StateGraph(dict)\ngraph.add_node(\"CHECK_DB\", node_check_db)\ngraph.add_node(\"DETERMINE_SCENES\", node_determine_scenes)\ngraph.add_node(\"RAG\", node_rag)\ngraph.add_node(\"GENERATE\", node_generate_story_and_image)\ngraph.add_node(\"SAVE_IF_REQ\", node_save_if_req)\ngraph.set_entry_point(\"CHECK_DB\")\ngraph.add_conditional_edges(\"CHECK_DB\", lambda s: \"SAVE_IF_REQ\" if s.get(\"found_in_db\") else \"DETERMINE_SCENES\",\n                            {\"SAVE_IF_REQ\":\"SAVE_IF_REQ\",\"DETERMINE_SCENES\":\"DETERMINE_SCENES\"})\ngraph.add_edge(\"DETERMINE_SCENES\",\"RAG\")\ngraph.add_edge(\"RAG\",\"GENERATE\")\ngraph.add_edge(\"GENERATE\",\"SAVE_IF_REQ\")\ngraph.add_edge(\"SAVE_IF_REQ\", END)\ncompiled = graph.compile()\n\ndef run_graph(place: str):\n    initial_state = {\"place\": place, \"error_flag\": False}\n    return compiled.invoke(initial_state)\n\n# ---------------- SCENE OUTPUT ----------------\ndef scene_output(state, idx):\n    outputs = \"\"\n    story_images = []\n    if state.get(\"error_flag\"):\n        outputs += f\"‚ö†Ô∏è Error: {state.get('error_message','Unknown error')}\\n\"\n\n    scenes = state.get(\"storybook\", [])\n    if scenes and 0 <= idx < len(scenes):\n        scene = scenes[idx]\n        outputs += f\"--- Scene {idx+1} ---\\nStory:\\n{scene.get('story','')}\\n\"\n        if scene.get(\"_is_end\"):\n            outputs += \"\\n--- The End ---\\n\"\n        img_path = scene.get(\"image\", \"\")\n        if img_path:\n            try:\n                img = Image.open(img_path)\n                story_images.append(img)\n            except:\n                pass\n    else:\n        outputs += \"No more scenes available.\\n\"\n\n    return outputs, story_images\n\n# ---------------- GRADIO CALLBACKS ----------------\ndef gradio_callback(place_name, scenes_state, idx_state):\n    state = run_graph(place_name)\n    scenes = state.get(\"storybook\", [])\n    current_index = 0\n    outputs, story_images = scene_output({\"storybook\": scenes}, current_index)\n    return outputs, story_images, scenes, current_index, gr.update(visible=True), gr.update(visible=True), gr.update(value=place_name)\n\ndef next_scene_callback(place_name, scenes, current_index):\n    scenes = list(scenes) if scenes else []\n\n    if not scenes:\n        return \"No scenes available. Generate first.\", [], scenes, 0\n\n    next_index = current_index + 1\n\n    if next_index < len(scenes):\n        if not scenes[next_index].get(\"ready\"):\n            return \"‚è≥ Next scene is still generating, please wait...\", [], scenes, current_index\n        current_index = next_index\n    else:\n        if all(scene.get(\"ready\") for scene in scenes):\n            end_scene = {\n                \"scene_desc\": \"End of Story\",\n                \"story\": \"üéâ This is the end of the story. Thank you for reading!\",\n                \"image\": \"\",\n                \"_is_end\": True,\n                \"ready\": True\n            }\n            scenes.append(end_scene)\n            current_index = len(scenes)-1\n        else:\n            return \"‚è≥ Still generating remaining scenes...\", [], scenes, current_index\n\n    outputs, story_images = scene_output({\"storybook\": scenes}, current_index)\n    return outputs, story_images, scenes, current_index\n\ndef back_scene_callback(place_name, scenes, current_index):\n    scenes = list(scenes) if scenes else []\n    if not scenes:\n        return \"No scenes available. Generate first.\", [], scenes, 0\n\n    current_index = max(current_index - 1, 0)\n    outputs, story_images = scene_output({\"storybook\": scenes}, current_index)\n    return outputs, story_images, scenes, current_index\n\n# ---------------- FEEDBACK ----------------\ndef save_feedback(place_name, scenes, mood_val, original_place_state):\n    scenes = list(scenes) if scenes else []\n    if mood_val == \"happy\":\n        for idx, scene in enumerate(scenes, start=1):\n            db_save_scene(place_name, scene.get(\"story\", \"\"), scene.get(\"image\", \"\"), idx, emotion=\"happy\")\n        msg = \"Saved to DB! Thank you for your happy feedback! üéâ\\nDo you want to visit a new place?\"\n        return gr.update(value=msg), gr.update(visible=False), gr.update(visible=False), gr.update(value=\"\"), gr.update(visible=True), gr.update(visible=True), gr.update(value=place_name)\n    else:\n        msg = \"Thank you for your feedback! (Not saved to DB). Would you like to try another place or regenerate images?\"\n        return gr.update(value=msg), gr.update(visible=False), gr.update(visible=False), gr.update(value=place_name), gr.update(visible=False), gr.update(visible=False), gr.update(value=place_name)\n\ndef visit_yes_callback(original_place):\n    msg = \"Great! Enter a new place and press Generate.\"\n    return gr.update(value=msg), gr.update(value=\"\"), gr.update(visible=False), gr.update(visible=False)\n\ndef visit_no_callback(original_place):\n    msg = \"Okay ‚Äî place restored. Press Generate to regenerate images for the same place.\"\n    return gr.update(value=msg), gr.update(value=original_place), gr.update(visible=False), gr.update(visible=False)\n\n# ---------------- GRADIO UI ----------------\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## üìñ Autonomous Kid Storybook Generator\")\n    with gr.Row():\n        place_input = gr.Textbox(label=\"Enter Place Name\")\n        run_btn = gr.Button(\"Generate Storybook\")\n        back_btn = gr.Button(\"Previous Scene\")\n        next_btn = gr.Button(\"Next Scene\")\n\n    with gr.Row():\n        output_text = gr.Textbox(label=\"Storybook Output\", lines=20)\n        output_images = gr.Gallery(label=\"Generated Images\", columns=1, height=\"auto\")\n\n    scenes_state = gr.State([])\n    idx_state = gr.State(0)\n    original_place_state = gr.State(\"\")  \n\n    mood = gr.Radio(label=\"How did you feel about this story?\", choices=[(\"üòä Happy\",\"happy\"),(\"üò¢ Sad\",\"sad\")], visible=True)\n    save_btn = gr.Button(\"Save my feedback\", visible=False)\n    visit_yes_btn = gr.Button(\"Yes ‚Äî visit a new place\", visible=False)\n    visit_no_btn = gr.Button(\"No ‚Äî keep this place\", visible=False)\n\n    run_btn.click(fn=lambda place_name, *_: gradio_callback(place_name, scenes_state, idx_state),\n                  inputs=[place_input, scenes_state, idx_state],\n                  outputs=[output_text, output_images, scenes_state, idx_state, mood, save_btn, place_input])\n\n    next_btn.click(fn=next_scene_callback, inputs=[place_input, scenes_state, idx_state],\n                   outputs=[output_text, output_images, scenes_state, idx_state])\n    back_btn.click(fn=back_scene_callback, inputs=[place_input, scenes_state, idx_state],\n                   outputs=[output_text, output_images, scenes_state, idx_state])\n\n    save_btn.click(fn=save_feedback, inputs=[place_input, scenes_state, mood, original_place_state],\n                   outputs=[output_text, mood, save_btn, place_input, visit_yes_btn, visit_no_btn, original_place_state])\n\n    mood.change(fn=lambda m, p: gr.update(visible=True) if m else gr.update(visible=False),\n                inputs=[mood, place_input], outputs=[save_btn])\n\n    visit_yes_btn.click(fn=visit_yes_callback, inputs=[original_place_state],\n                        outputs=[output_text, place_input, visit_yes_btn, visit_no_btn])\n    visit_no_btn.click(fn=visit_no_callback, inputs=[original_place_state],\n                       outputs=[output_text, place_input, visit_yes_btn, visit_no_btn])\n\ndemo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T15:51:59.829759Z","iopub.execute_input":"2025-12-02T15:51:59.830842Z","iopub.status.idle":"2025-12-02T15:52:04.655684Z","shell.execute_reply.started":"2025-12-02T15:51:59.830814Z","shell.execute_reply":"2025-12-02T15:52:04.654627Z"}},"outputs":[],"execution_count":null}]}